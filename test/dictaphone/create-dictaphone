#!/usr/bin/env bash
# Create a Ruby2JS dictaphone demo with speech-to-text transcription
# Usage: create-dictaphone [app-name]
#
# Creates a Rails app with a Clip model, audio recording via MediaRecorder,
# and client-side transcription using Whisper.js (Transformers.js).
#
# Features:
# - Audio recording with MediaRecorder API
# - Speech-to-text transcription via Whisper (runs entirely in browser)
# - Active Storage for audio file persistence
# - Real-time updates via Turbo Streams
# - ~75MB model download on first use (cached in IndexedDB)

set -e

APP_NAME="${1:-dictaphone}"

echo "Creating Ruby2JS dictaphone: $APP_NAME"
echo ""

# Create Rails app (minimal, with Tailwind for nice styling)
rails new "$APP_NAME" --skip-git --skip-docker --css tailwind
cd "$APP_NAME"

# Generate Clip model
# - name: user-provided name for the clip
# - transcript: text from speech-to-text
# - duration: length in seconds
# - audio: Active Storage attachment (not a column)
rails generate model Clip name:string transcript:text duration:float

# Install Active Storage (creates migrations for blobs and attachments tables)
bin/rails active_storage:install

# Create database
rails db:create

# Update Clip model with Active Storage and broadcasting
cat > app/models/clip.rb << 'MODEL'
class Clip < ApplicationRecord
  has_one_attached :audio

  validates :name, presence: true

  broadcasts_to -> { "clips" }, inserts_by: :prepend
end
MODEL

# Create ClipsController
mkdir -p app/controllers
cat > app/controllers/clips_controller.rb << 'CONTROLLER'
class ClipsController < ApplicationController
  def index
    @clips = Clip.order(created_at: :desc)
  end

  def show
    @clip = Clip.find(params[:id])
  end

  def create
    @clip = Clip.new(clip_params)

    if @clip.save
      respond_to do |format|
        format.turbo_stream
        format.html { redirect_to clips_path }
      end
    else
      redirect_to clips_path, alert: "Could not save clip."
    end
  end

  def update
    @clip = Clip.find(params[:id])

    if @clip.update(clip_params)
      respond_to do |format|
        format.turbo_stream
        format.html { redirect_to clips_path, notice: "Transcript updated." }
      end
    else
      redirect_to clips_path, alert: "Could not update clip."
    end
  end

  def destroy
    @clip = Clip.find(params[:id])
    @clip.audio.purge if @clip.audio.attached?
    @clip.destroy

    respond_to do |format|
      format.turbo_stream
      format.html { redirect_to clips_path, notice: "Clip deleted." }
    end
  end

  private

  def clip_params
    params.require(:clip).permit(:name, :transcript, :duration, :audio)
  end
end
CONTROLLER

# Set up routes
cat > config/routes.rb << 'ROUTES'
Rails.application.routes.draw do
  root "clips#index"
  resources :clips, only: [:index, :show, :create, :update, :destroy]
end
ROUTES

# Create views directory
mkdir -p app/views/clips

# Create index view with recording UI
cat > app/views/clips/index.html.erb << 'VIEW'
<%# Subscribe to real-time clip updates %>
<%= turbo_stream_from "clips" %>

<div class="container mx-auto px-4 py-8" data-controller="dictaphone">
  <h1 class="text-3xl font-bold mb-2">Dictaphone</h1>
  <p class="text-gray-600 mb-8">Record audio and get automatic transcriptions powered by Whisper AI.</p>

  <!-- Model loading status -->
  <div data-dictaphone-target="status" class="mb-6 p-4 bg-blue-50 text-blue-800 rounded-lg">
    Loading Whisper model...
  </div>

  <!-- Recording controls -->
  <div class="mb-8 p-6 bg-gray-100 rounded-lg">
    <div class="flex gap-4 items-center mb-4">
      <button data-dictaphone-target="record"
              data-action="click->dictaphone#toggleRecording"
              disabled
              class="bg-red-600 text-white px-6 py-3 rounded-full font-semibold hover:bg-red-700 transition disabled:opacity-50 disabled:cursor-not-allowed flex items-center gap-2">
        <span data-dictaphone-target="recordIcon">&#9679;</span>
        <span data-dictaphone-target="recordLabel">Record</span>
      </button>

      <!-- Recording timer -->
      <span data-dictaphone-target="timer" class="text-2xl font-mono text-gray-600 hidden">
        00:00
      </span>

      <!-- Audio level visualizer -->
      <div data-dictaphone-target="visualizer" class="flex-1 h-8 bg-gray-200 rounded hidden">
        <div data-dictaphone-target="level" class="h-full bg-green-500 rounded transition-all duration-100" style="width: 0%"></div>
      </div>
    </div>

    <!-- Playback and save form (shown after recording) -->
    <div data-dictaphone-target="preview" class="hidden">
      <audio data-dictaphone-target="audio" controls class="w-full mb-4"></audio>

      <div data-dictaphone-target="transcribing" class="mb-4 p-3 bg-yellow-50 text-yellow-800 rounded hidden">
        Transcribing audio...
      </div>

      <%= form_with url: clips_path, method: :post,
                    data: { dictaphone_target: "form", action: "submit->dictaphone#save" },
                    class: "space-y-4" do |f| %>
        <input type="hidden" name="clip[duration]" data-dictaphone-target="duration">
        <%# Note: audioData target kept for resetUI compatibility %>
        <input type="hidden" data-dictaphone-target="audioData">

        <div>
          <label class="block text-sm font-medium text-gray-700 mb-1">Name</label>
          <input type="text" name="clip[name]"
                 data-dictaphone-target="name"
                 placeholder="My recording"
                 class="w-full border rounded-lg p-3">
        </div>

        <div>
          <label class="block text-sm font-medium text-gray-700 mb-1">Transcript</label>
          <textarea name="clip[transcript]"
                    data-dictaphone-target="transcript"
                    rows="4"
                    placeholder="Transcription will appear here..."
                    class="w-full border rounded-lg p-3"></textarea>
        </div>

        <div class="flex gap-4">
          <%= f.submit "Save Clip",
                 class: "bg-green-600 text-white px-6 py-3 rounded-lg font-semibold hover:bg-green-700 transition cursor-pointer" %>
          <button type="button"
                  data-action="click->dictaphone#discard"
                  class="bg-gray-400 text-white px-6 py-3 rounded-lg font-semibold hover:bg-gray-500 transition">
            Discard
          </button>
        </div>
      <% end %>
    </div>
  </div>

  <!-- Clips list -->
  <div>
    <h2 class="text-xl font-semibold mb-4">Your Clips</h2>
    <div id="clips" class="space-y-4">
      <%= render @clips %>
      <p id="empty-message" class="text-gray-500 text-center py-12 hidden only:block">
        No clips yet. Record something to get started!
      </p>
    </div>
  </div>
</div>
VIEW

# Create clip partial
cat > app/views/clips/_clip.html.erb << 'PARTIAL'
<div id="<%= dom_id(clip) %>" class="bg-white rounded-lg shadow-md p-4">
  <div class="flex justify-between items-start mb-3">
    <div>
      <h3 class="font-semibold text-lg"><%= clip.name %></h3>
      <span class="text-gray-500 text-sm">
        <%= clip.created_at.strftime("%b %d, %Y at %H:%M") %>
        <% if clip.duration %>
          &middot; <%= clip.duration.round(1) %>s
        <% end %>
      </span>
    </div>
    <%= button_to clip_path(clip), method: :delete,
        class: "text-gray-400 hover:text-red-600",
        data: { turbo_confirm: "Delete this clip?" } do %>
      <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16" />
      </svg>
    <% end %>
  </div>

  <% if clip.audio.attached? %>
    <audio controls class="w-full mb-3">
      <source src="<%= clip.audio.url %>" type="<%= clip.audio.content_type %>">
    </audio>
  <% end %>

  <% if clip.transcript.present? %>
    <div class="bg-gray-50 rounded p-3">
      <p class="text-gray-700 text-sm"><%= clip.transcript %></p>
    </div>
  <% else %>
    <p class="text-gray-400 text-sm italic">No transcript available</p>
  <% end %>
</div>
PARTIAL

# Create turbo stream responses
cat > app/views/clips/create.turbo_stream.erb << 'TURBO'
<%= turbo_stream.prepend "clips", @clip %>
TURBO

cat > app/views/clips/update.turbo_stream.erb << 'TURBO'
<%= turbo_stream.replace @clip %>
TURBO

cat > app/views/clips/destroy.turbo_stream.erb << 'TURBO'
<%= turbo_stream.remove @clip %>
TURBO

# Create Stimulus dictaphone controller (Ruby)
mkdir -p app/javascript/controllers
cat > app/javascript/controllers/dictaphone_controller.rb << 'STIMULUS'
# Whisper speech-to-text via Transformers.js
import ["pipeline", "read_audio"], from: '@xenova/transformers'
# Active Storage for audio file persistence
import ["initActiveStorage"], from: 'juntos:active-storage'
# Clip model for client-side persistence
import ["Clip"], from: 'juntos:models'

class DictaphoneController < Stimulus::Controller
  def connect
    @transcriber = nil
    @mediaRecorder = nil
    @audioContext = nil
    @analyser = nil
    @chunks = []
    @startTime = nil
    @timerInterval = nil
    @isRecording = false
    @audioBlob = nil      # Store audio blob for Active Storage
    @audioDuration = nil  # Store recording duration

    initStorage()
    loadModel()
  end

  def disconnect
    stopRecording() if @isRecording
    @timerInterval && clearInterval(@timerInterval)
  end

  # Initialize Active Storage
  async def initStorage
    begin
      await initActiveStorage()
    rescue => error
      console.error("Failed to initialize Active Storage:", error)
    end
  end

  # Load the Whisper model (downloads ~75MB on first use, cached after)
  async def loadModel
    begin
      @transcriber = await pipeline(
        'automatic-speech-recognition',
        'Xenova/whisper-tiny.en',
        progress_callback: ->(progress) {
          if progress.status == 'progress' && progress.progress
            pct = progress.progress.round
            statusTarget.textContent = "Loading Whisper model... #{pct}%"
          elsif progress.status == 'ready'
            statusTarget.textContent = "Model loaded!"
          end
        }
      )

      statusTarget.textContent = "Ready to record"
      statusTarget.classList.remove('bg-blue-50', 'text-blue-800')
      statusTarget.classList.add('bg-green-50', 'text-green-800')
      recordTarget.disabled = false
    rescue => error
      console.error("Failed to load Whisper model:", error)
      statusTarget.textContent = "Failed to load model. Please refresh and try again."
      statusTarget.classList.remove('bg-blue-50', 'text-blue-800')
      statusTarget.classList.add('bg-red-50', 'text-red-800')
    end
  end

  def toggleRecording
    if @isRecording
      stopRecording()
    else
      startRecording()
    end
  end

  async def startRecording
    begin
      stream = await navigator.mediaDevices.getUserMedia(audio: true)

      # Set up audio analysis for visualizer
      @audioContext = AudioContext.new
      source = @audioContext.createMediaStreamSource(stream)
      @analyser = @audioContext.createAnalyser()
      @analyser.fftSize = 256
      source.connect(@analyser)

      # Determine best supported format
      mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') ?
                   'audio/webm;codecs=opus' : 'audio/mp4'

      @mediaRecorder = MediaRecorder.new(stream, mimeType: mimeType)
      @chunks = []

      @mediaRecorder.ondataavailable = ->(e) {
        @chunks.push(e.data) if e.data.size > 0
      }

      @mediaRecorder.onstop = -> { handleRecordingComplete() }

      @mediaRecorder.start(100) # Collect data every 100ms
      @isRecording = true
      @startTime = Date.now()

      # Update UI
      recordLabelTarget.textContent = "Stop"
      recordTarget.classList.remove('bg-red-600', 'hover:bg-red-700')
      recordTarget.classList.add('bg-gray-800', 'hover:bg-gray-900')
      timerTarget.classList.remove('hidden')
      visualizerTarget.classList.remove('hidden')
      previewTarget.classList.add('hidden')

      # Start timer and visualizer
      updateTimer()
      @timerInterval = setInterval(-> { updateTimer() }, 100)
      visualize()

    rescue => error
      console.error("Failed to start recording:", error)
      statusTarget.textContent = "Microphone access denied. Please allow microphone access."
      statusTarget.classList.remove('bg-green-50', 'text-green-800')
      statusTarget.classList.add('bg-red-50', 'text-red-800')
    end
  end

  def stopRecording
    return unless @mediaRecorder && @isRecording

    @mediaRecorder.stop()
    @mediaRecorder.stream.getTracks().each { |track| track.stop() }
    @isRecording = false

    clearInterval(@timerInterval) if @timerInterval
    @audioContext.close() if @audioContext

    # Update UI
    recordLabelTarget.textContent = "Record"
    recordTarget.classList.remove('bg-gray-800', 'hover:bg-gray-900')
    recordTarget.classList.add('bg-red-600', 'hover:bg-red-700')
    timerTarget.classList.add('hidden')
    visualizerTarget.classList.add('hidden')
  end

  def updateTimer
    return unless @startTime
    elapsed = (Date.now() - @startTime) / 1000
    mins = Math.floor(elapsed / 60).toString().padStart(2, '0')
    secs = Math.floor(elapsed % 60).toString().padStart(2, '0')
    timerTarget.textContent = "#{mins}:#{secs}"
  end

  def visualize
    return unless @analyser && @isRecording

    dataArray = Uint8Array.new(@analyser.frequencyBinCount)
    @analyser.getByteFrequencyData(dataArray)

    # Calculate average level
    sum = 0
    dataArray.each { |value| sum += value }
    average = sum / dataArray.length
    percentage = Math.min(100, (average / 128) * 100)

    levelTarget.style.width = "#{percentage}%"

    requestAnimationFrame(-> { visualize() }) if @isRecording
  end

  async def handleRecordingComplete
    # Create blob from chunks
    mimeType = @mediaRecorder.mimeType
    @audioBlob = Blob.new(@chunks, type: mimeType)
    @audioDuration = (Date.now() - @startTime) / 1000

    # Create playback URL
    audioUrl = URL.createObjectURL(@audioBlob)
    audioTarget.src = audioUrl

    # Set duration in form
    durationTarget.value = @audioDuration.toFixed(2)

    # Generate default name
    now = Date.new
    nameTarget.value = "Recording #{now.toLocaleDateString()} #{now.toLocaleTimeString()}"

    # Show preview
    previewTarget.classList.remove('hidden')

    # Start transcription
    transcribe(@audioBlob)
  end

  async def transcribe(blob)
    return unless @transcriber

    transcribingTarget.classList.remove('hidden')
    transcriptTarget.value = ""
    transcriptTarget.placeholder = "Transcribing..."

    begin
      # Create object URL for read_audio
      audioUrl = URL.createObjectURL(blob)

      # Convert audio to Float32Array at 16kHz (Whisper's expected format)
      audioData = await read_audio(audioUrl, 16000)

      # Clean up object URL
      URL.revokeObjectURL(audioUrl)

      # Transcribe
      result = await @transcriber.call(audioData)

      transcriptTarget.value = result.text.strip()
      transcriptTarget.placeholder = "Transcription will appear here..."

    rescue => error
      console.error("Transcription failed:", error)
      transcriptTarget.placeholder = "Transcription failed. You can type manually."
    ensure
      transcribingTarget.classList.add('hidden')
    end
  end

  async def save(event)
    event.preventDefault()

    return unless @audioBlob

    begin
      # Create clip record first
      clip = await Clip.create(
        name: nameTarget.value || "Untitled Recording",
        transcript: transcriptTarget.value,
        duration: parseFloat(durationTarget.value) || @audioDuration
      )

      # Attach audio via Active Storage
      extension = @audioBlob.type.include?('webm') ? 'webm' : 'm4a'
      await clip.audio.attach(@audioBlob,
        filename: "#{clip.name.gsub(/[^a-z0-9]/i, '_')}.#{extension}",
        content_type: @audioBlob.type
      )

      console.log("Clip saved with audio attachment:", clip.id)

      # Replace the initially-broadcast clip (which had no audio) with the full version
      await clip.broadcast_replace_to("clips")

      resetUI()

    rescue => error
      console.error("Failed to save clip:", error)
      statusTarget.textContent = "Failed to save clip. Please try again."
      statusTarget.classList.remove('bg-green-50', 'text-green-800')
      statusTarget.classList.add('bg-red-50', 'text-red-800')
    end
  end

  def discard
    resetUI()
  end

  def resetUI
    # Revoke object URL
    URL.revokeObjectURL(audioTarget.src) if audioTarget.src

    # Clear stored blob
    @audioBlob = nil
    @audioDuration = nil

    # Clear form
    audioTarget.src = ""
    audioDataTarget.value = ""
    durationTarget.value = ""
    nameTarget.value = ""
    transcriptTarget.value = ""

    # Hide preview
    previewTarget.classList.add('hidden')

    # Reset status
    statusTarget.textContent = "Ready to record"
    statusTarget.classList.remove('bg-red-50', 'text-red-800')
    statusTarget.classList.add('bg-green-50', 'text-green-800')
  end
end
STIMULUS

# Rebuild Tailwind CSS
bin/rails tailwindcss:build

# Create seeds (empty - clips come from recording)
cat > db/seeds.rb << 'SEEDS'
# Dictaphone seeds
# Clips are created through the recording interface
puts "Dictaphone ready - start recording!"
SEEDS

# Create ruby2js.yml with Whisper dependency
cat > config/ruby2js.yml << 'CONFIG'
# Dictaphone demo configuration
dependencies:
  "@xenova/transformers": "^2.17.0"
CONFIG

# Create integration test for clips
cat > test/clips.test.mjs << 'TEST'
import { describe, it, expect, beforeAll } from 'vitest';

// Initialize Active Storage before tests
import { initActiveStorage } from 'juntos:active-storage';

beforeAll(async () => {
  await initActiveStorage();
});

describe('Clip Model', () => {
  it('creates a clip with valid attributes', async () => {
    const { Clip } = await import('juntos:models');

    const clip = await Clip.create({
      name: 'Test Recording',
      transcript: 'Hello, this is a test.',
      duration: 5.5
    });

    expect(clip.id).toBeDefined();
    expect(clip.name).toBe('Test Recording');
    expect(clip.transcript).toBe('Hello, this is a test.');
    expect(clip.duration).toBe(5.5);
  });

  it('validates name presence', async () => {
    const { Clip } = await import('juntos:models');

    const clip = new Clip({ name: '', transcript: 'Some text' });
    const saved = await clip.save();

    expect(saved).toBe(false);
    expect(clip.errors.name).toBeDefined();
  });

  it('allows clip without transcript', async () => {
    const { Clip } = await import('juntos:models');

    const clip = await Clip.create({
      name: 'No transcript clip'
    });

    expect(clip.id).toBeDefined();
    expect(clip.transcript).toBeUndefined();
  });
});

describe('Clip Active Storage', () => {
  it('attaches audio to a clip', async () => {
    const { Clip } = await import('juntos:models');

    const clip = await Clip.create({
      name: 'Audio Test',
      duration: 3.0
    });

    // Create a test audio blob
    const audioData = new Uint8Array([0, 1, 2, 3, 4, 5]);
    const audioBlob = new Blob([audioData], { type: 'audio/webm' });

    // Attach the audio
    await clip.audio.attach(audioBlob, {
      filename: 'test.webm',
      content_type: 'audio/webm'
    });

    // Verify attachment
    expect(await clip.audio.attached()).toBe(true);
    expect(await clip.audio.filename()).toBe('test.webm');
    expect(await clip.audio.contentType()).toBe('audio/webm');
    expect(await clip.audio.byteSize()).toBe(6);
  });

  it('downloads attached audio', async () => {
    const { Clip } = await import('juntos:models');

    const clip = await Clip.create({ name: 'Download Test' });

    const originalData = new Uint8Array([10, 20, 30, 40, 50]);
    const audioBlob = new Blob([originalData], { type: 'audio/webm' });

    await clip.audio.attach(audioBlob);

    // Download and verify
    const downloaded = await clip.audio.download();
    expect(downloaded).toBeDefined();

    const buffer = typeof downloaded.arrayBuffer === 'function'
      ? await downloaded.arrayBuffer()
      : await new Response(downloaded).arrayBuffer();
    const downloadedArray = new Uint8Array(buffer);
    expect(downloadedArray.length).toBe(5);
    expect(downloadedArray[0]).toBe(10);
  });

  it('purges audio attachment', async () => {
    const { Clip } = await import('juntos:models');

    const clip = await Clip.create({ name: 'Purge Test' });

    const audioBlob = new Blob([1, 2, 3], { type: 'audio/webm' });
    await clip.audio.attach(audioBlob);

    expect(await clip.audio.attached()).toBe(true);

    // Purge the attachment
    await clip.audio.purge();

    expect(await clip.audio.attached()).toBe(false);
  });

  it('replaces existing attachment', async () => {
    const { Clip } = await import('juntos:models');

    const clip = await Clip.create({ name: 'Replace Test' });

    // Attach first audio
    const audio1 = new Blob([1, 1, 1], { type: 'audio/webm' });
    await clip.audio.attach(audio1, { filename: 'first.webm' });

    expect(await clip.audio.filename()).toBe('first.webm');

    // Attach second audio (should replace first)
    const audio2 = new Blob([2, 2, 2, 2], { type: 'audio/mp4' });
    await clip.audio.attach(audio2, { filename: 'second.m4a' });

    expect(await clip.audio.filename()).toBe('second.m4a');
    expect(await clip.audio.byteSize()).toBe(4);
  });
});

describe('ClipsController', () => {
  it('index action returns list', async () => {
    const { Clip } = await import('juntos:models');
    const { ClipsController } = await import('../app/controllers/clips_controller.rb');

    await Clip.create({ name: 'Test Clip', transcript: 'Test content' });

    const context = {
      params: {},
      flash: { get: () => '', consumeNotice: () => ({ present: false }), consumeAlert: () => '' },
      contentFor: {}
    };

    const html = await ClipsController.index(context);
    expect(html).toContain('Dictaphone');
  });

  it('create action adds a new clip', async () => {
    const { Clip } = await import('juntos:models');
    const { ClipsController } = await import('../app/controllers/clips_controller.rb');

    const context = {
      params: {},
      flash: { set: () => {} },
      contentFor: {},
      request: { headers: { get: () => 'text/html' } }
    };

    const params = {
      clip: {
        name: 'New Recording',
        transcript: 'This is the transcript.',
        duration: 10.0
      }
    };

    const result = await ClipsController.create(context, params);

    expect(result.redirect).toBeDefined();

    const clips = await Clip.all();
    expect(clips.length).toBe(1);
    expect(clips[0].name).toBe('New Recording');
  });
});
TEST

# Make script executable
chmod +x "$0" 2>/dev/null || true

echo ""
echo "Dictaphone created: $APP_NAME/"
echo ""
echo "Features:"
echo "  - Audio recording via MediaRecorder API"
echo "  - Speech-to-text transcription via Whisper AI (runs in browser)"
echo "  - ~75MB model download on first use (cached in IndexedDB)"
echo "  - Real-time updates via Turbo Streams"
echo "  - Active Storage for audio file persistence"
echo ""
echo "To run with Juntos (transpiled to JavaScript):"
echo "  cd $APP_NAME"
echo "  bin/juntos dev -d dexie              # Browser with IndexedDB"
echo ""
echo "To deploy to GitHub Pages:"
echo "  bin/juntos build -t browser -d dexie"
echo "  # Upload dist/ to GitHub Pages"
echo ""
echo "Note: First load downloads the Whisper model (~75MB)."
echo "Subsequent visits use the cached model from IndexedDB."
echo ""
echo "To run with Rails (requires ActiveStorage setup):"
echo "  cd $APP_NAME"
echo "  bin/rails active_storage:install"
echo "  bin/rails db:prepare"
echo "  bin/rails server"
echo ""
